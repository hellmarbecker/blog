---
layout: post
title:  "Streaming Events from Redpanda Cloud into Imply Polaris"
categories: blog druid imply polaris saas eventstreaming redpanda kafka
---

[Redpanda](https://redpanda.com/) offers a data streaming platform with a Kafka comaptible API. There are a variety of deployment options - you can run Redpanda on premise, but today I am going to look at Redpanda's fully managed cloud service, which I want to connect to Imply's [Polaris](https://imply.io/imply-polaris/) database-as-a-service, which offers fast, near realtime analytics based on [Apache Druid](https://druid.apache.org/).

Polaris has recently upgraded with connectivity to multiple streaming services. Today I am going to show how to build a realtime analytics pipeline using Redpanda Cloud and Imply Polaris, that is, only fully managed cloud services.

In this tutorial, you will learn how to create a connection to a dedicated Redpanda cluster and to populate a Polaris table with realtime data using the Polaris API, which allows for automatied scripting and zero touch configuration.

As prerequisites, you will need:

- a Redpanda cloud account (a 15 day free trial is available)
- an Imply Polaris account (you can sign up for a 30 day trial).

## Setting up Redpanda

### Create a cluster

I am assuming that you have a Redpanda Cloud account created. 

In Redpanda Cloud, a cluster exists in a namespace, and you can have multiple namespaces inside an organization. Create a namespace, and in that namespace reate a cluster according to the [documentation](https://docs.redpanda.com/docs/deploy/deployment-option/cloud/create-dedicated-cloud-cluster-aws/). The smallest cluster option is sufficient for this experiment.

I created a cluster in AWS, single region and single AZ. I am not using VPC peering - make sure your cluster is connected to the public Internet. While VPC peering between Redpanda Cloud and Imply Polaris is possible, that is a story for another blog.

Redpanda will suggest a unique (3-word) cluster name, but you can of course pick your own.

### Create a topic

Creating a topic is straightforward in Redpanda:

![](/assets/2023-02-14-01-redpanda-topic.jpg)

Create a topic named `pizza`, and leave all settings at the defaults.

### Create a user and password

In Redpanda Cloud, go to the `Security` tab and create a new (data plane) user. You can enter your own password or use the autogenerated one. Make sure to copy the password because you will need it later. Set the security protocol to `SCRAM-SHA-512`.

![](/assets/2023-02-14-02-redpanda-user.jpg)

### Set up ACLs

You have to set up ACLs to grant access rights. Click the name of the user you just created, and in the popup click `Allow all operations`:

![](/assets/2023-02-14-03-redpanda-acl.jpg)

This gives my user full access rights. In a production setup, you wouldnot do that: you would restrict that user's access rights to a topic or group of topics.

## Populating the Redpanda Topic

Once more, I am using [Francesco's pizza simulator](https://github.com/aiven/python-fake-data-producer-for-apache-kafka). I have described that [in a previous blog](/2022/11/23/processing-nested-json-data-and-kafka-metadata-in-apache-druid/).

Find the bootstrap server from the Redpanda overview page:

![](/assets/2023-02-14-04-redpanda-overview.jpg)

Check out the data generator repository, and using the bootstrap address, and the username and password you just configured, start the data generator like so:

```bash
python3 main.py \
  --security-protocol SASL_SSL \
  --sasl-mechanism SCRAM-SHA-512 \
  --username '<Redpanda username from above>' \
  --password '<Redpanda password>' \
  --host <Redpanda bootstrap server> \
  --port 9092 \
  --topic-name pizza \
  --nr-messages 0 \
  --max-waiting-time 0
```

After a moment, you can see the incoming messages in the Redpanda console too:

![](/assets/2023-02-14-05-redpanda-messages.jpg)

## Setting up Imply Polaris

### Create an API key

Creating a Kafka Pull connection

Adding credentials to the connection

Testing the connection

Creating a Polaris table

Setting the table schema

Firing up the ingestion job

